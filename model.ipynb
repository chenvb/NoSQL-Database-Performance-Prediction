{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pandarallel import pandarallel\n",
    "import gc\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pty\n",
    "import signal\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso,ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import gpytorch\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import norm,expon\n",
    "import scipy.stats as stats\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_recall_curve, auc, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# import pymc3 as pm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowsize = 10000\n",
    "basepath='/benchmark/YCSB/result/cassandra/repeat/'\n",
    "def zero_ratio(series):\n",
    "    total_count = len(series)\n",
    "    zero_count = (series == 0).sum()\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return zero_count / total_count\n",
    "    \n",
    "def custom_percentiles4(series):\n",
    "    percentile_columns = [0.25,0.5,0.75,0.95]\n",
    "    if len(series) == 0:\n",
    "        return pd.Series([-1] * len(percentile_columns), index=percentile_columns)\n",
    "    else:\n",
    "        return pd.Series(series.quantile(percentile_columns), index=percentile_columns)\n",
    "    \n",
    "def custom_percentiles_tail(series):\n",
    "    percentile_columns = [0.25,0.5,0.75,0.9,0.95,0.99]\n",
    "    if len(series) == 0:\n",
    "        return pd.Series([0] * len(percentile_columns), index=percentile_columns)\n",
    "    else:\n",
    "        return pd.Series(series.quantile(percentile_columns), index=percentile_columns)\n",
    "    \n",
    "def custom_percentiles2(series):\n",
    "    percentile_columns = [0.5, 0.95]\n",
    "    if len(series) == 0:\n",
    "        return pd.Series([-1] * len(percentile_columns), index=percentile_columns)\n",
    "    else:\n",
    "        return pd.Series(series.quantile(percentile_columns), index=percentile_columns)\n",
    "\n",
    "def quantile_bins(group, num_bins):\n",
    "    if len(group) < num_bins:\n",
    "        return pd.Series(np.linspace(0, num_bins - 1, len(group)).astype(int))\n",
    "    else:\n",
    "        return pd.qcut(group, num_bins, labels=False, duplicates='drop')\n",
    "    \n",
    "def key_rangecout(x):\n",
    "    return x.nunique()/x.count()\n",
    "\n",
    "def filter_firstwindow(df):\n",
    "    df['subgroup'] = (df.groupby('group').cumcount() // windowsize) \n",
    "    \n",
    "    df['global_subgroup'] = df.groupby(['group', 'subgroup']).ngroup()\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # df = df.drop(columns=['group', 'subgroup'],axis=1)\n",
    "    df = df.rename(columns={'group': 'group_raw'})\n",
    "    df = df.rename(columns={'global_subgroup': 'group'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_ape(y_true, y_pred):\n",
    "    \"\"\" Calculate Absolute Percentage Error for each prediction \"\"\"\n",
    "    epsilon = 1e-7\n",
    "    return np.abs((y_true - y_pred) / (y_true + epsilon))\n",
    "\n",
    "def calculate_mape(row):\n",
    "    actual = row[:-2]  # 最后两列是均值和方差，不包括在实际值中\n",
    "    mean_value = row['mean']\n",
    "    ape = calculate_ape(actual, mean_value)\n",
    "    mape = ape.mean()\n",
    "    return mape\n",
    "\n",
    "#划分滑动窗口\n",
    "def create_sliding_window(features, b_errors, window_size,X,y):\n",
    "    for i in range(len(features) - window_size):\n",
    "        X.append(features[i:(i + window_size)])\n",
    "        y.append(b_errors[i + window_size])\n",
    "    return X, y\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "def getfeature(recordfile='record6_0.txt',distancefile='distance6_0.txt',filepath = '/benchmark/YCSB/result/cassandra/repeat/',type = 'full'):\n",
    "    df = pd.DataFrame()\n",
    "    separator_count = 0\n",
    "    current_group_number = 0\n",
    "    #分块读\n",
    "    chunksize = 10000000\n",
    "    #读取文件，使用逗号分隔\n",
    "    \n",
    "    \n",
    "    for dftmp in pd.read_csv(filepath+recordfile, sep=',', header=None, names=['key', 'value','optype'], dtype={0: str, 1: np.float64,2:str},chunksize=chunksize):\n",
    "\n",
    "        # 标记分隔符行\n",
    "        dftmp['is_separator'] = dftmp['key'] == '**'\n",
    "\n",
    "        separator_count += dftmp['is_separator'].sum()\n",
    "        print(separator_count)\n",
    "        # # 使用 cumsum 创建分组标签\n",
    "        dftmp['group'] = dftmp['is_separator'].cumsum() + current_group_number\n",
    "        \n",
    "        # 更新当前group数量\n",
    "        current_group_number = dftmp['group'].iloc[-1]\n",
    "\n",
    "        # # 移除分隔符行\n",
    "        dftmp = dftmp[~dftmp['is_separator']]\n",
    "\n",
    "        # 转换第二列为数值类型（如果尚未转换）\n",
    "        dftmp['value'] = pd.to_numeric(dftmp['value'], errors='coerce')\n",
    "        dftmp['optype'] = pd.to_numeric(dftmp['optype'], errors='coerce')\n",
    "\n",
    "        print(f\"dftmp.shape:{dftmp.shape}\")\n",
    "        \n",
    "        #分块的合并\n",
    "        df = pd.concat([df,dftmp],ignore_index=True)\n",
    "\n",
    "\n",
    "        \n",
    "    # percentile_columns = ['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%']\n",
    "    percentile_columns_tail =['25%', '50%', '75%', '90%', '95%', '99%']\n",
    "    percentile_columns =['25%', '50%', '75%', '100%']\n",
    "    \n",
    "    #\n",
    "    # with open(filepath + recordfile) as f:\n",
    "    #     lines = f.readlines()\n",
    "    # # 处理每一行并转为 DataFrame\n",
    "    # data = [line.strip().split(',') for line in lines if line.strip()]  # 去除空行\n",
    "    # df = pd.DataFrame(data, columns=['key', 'value', 'optype'])\n",
    "    \n",
    "    \n",
    "    fe = pd.DataFrame()\n",
    "    fe=pd.read_csv(filepath+distancefile, header=None,names=['Unique_Value_Sum'])\n",
    "    df = df[:len(fe)]\n",
    "    df['group'] = 0\n",
    "    print(f\"df length:{len(df)}-------distancelength:{len(fe)}\")\n",
    "    df['Unique_Value_Sum']=fe['Unique_Value_Sum'].values\n",
    "\n",
    "    df = filter_firstwindow(df)\n",
    "    all_groups = df['group'].unique()\n",
    "\n",
    "    result_percentiles_sum = df[(df['optype'] == 0) & (df['value'] > 0)].groupby(['group'])['Unique_Value_Sum'].apply(custom_percentiles_tail)\n",
    "    if result_percentiles_sum.empty:\n",
    "        result_percentiles_sum = pd.DataFrame(0, index=all_groups, columns=percentile_columns_tail).reset_index()\n",
    "        result_percentiles_sum.rename(columns={'index': 'group'}, inplace=True)\n",
    "    else:\n",
    "        result_percentiles_sum = result_percentiles_sum.unstack(level=-1).reset_index()\n",
    "    percentile_columns_tmp = [i+'sum' for i in percentile_columns_tail]\n",
    "    result_percentiles_sum.columns = ['group']+ percentile_columns_tmp\n",
    "\n",
    "    result_nullpercentiles_sum = df[(df['optype'] == 0) & (df['value'] == 0)].groupby(['group'])['Unique_Value_Sum'].apply(custom_percentiles_tail)\n",
    "    if result_nullpercentiles_sum.empty:\n",
    "        result_nullpercentiles_sum = pd.DataFrame(0, index=all_groups, columns=percentile_columns_tail).reset_index()\n",
    "        result_nullpercentiles_sum.rename(columns={'index': 'group'}, inplace=True)\n",
    "    else:\n",
    "        result_nullpercentiles_sum = result_nullpercentiles_sum.unstack(level=-1).reset_index()\n",
    "    percentile_columns_tmp = [i+'null_sum' for i in percentile_columns_tail]\n",
    "    result_nullpercentiles_sum.columns = ['group']+ percentile_columns_tmp\n",
    "\n",
    "    get_num = df[(df['optype'] == 0) & (df['value'] !=0)].groupby('group')['value'].agg('count').reset_index()\n",
    "    get_num.columns=['group','get_ct']\n",
    "    getnull_num = df[(df['optype'] == 0) & (df['value'] ==0)].groupby('group')['value'].agg('count').reset_index()\n",
    "    getnull_num.columns=['group','getnull_ct']\n",
    "    set_num = df[df['optype'] == 1].groupby('group')['value'].agg('count').reset_index()\n",
    "    set_num.columns=['group','set_ct']\n",
    "\n",
    "    # 读写比\n",
    "    result_zero_ratio = df.groupby('group')['optype'].agg('mean').reset_index()\n",
    "    result_get_zero_ratio = df[df['optype'] == 0].groupby('group')['value'].agg([zero_ratio]).reset_index()\n",
    "    get_key_range=df[(df['optype'] == 0) & (df['value'] > 0)].groupby('group')['key'].apply(key_rangecout).reset_index()\n",
    "\n",
    "\n",
    "    # set value percentiles\n",
    "    result_percentiles_set = df[df['optype'] == 1].groupby('group')['value'].apply(custom_percentiles4)\n",
    "    if result_percentiles_set.empty:\n",
    "        result_percentiles_set = pd.DataFrame(0, index=all_groups, columns=percentile_columns).reset_index()\n",
    "        result_percentiles_set.rename(columns={'index': 'group'}, inplace=True)\n",
    "    else:\n",
    "        result_percentiles_set = result_percentiles_set.unstack(level=-1).reset_index()\n",
    "    percentile_columns_tmp = [i+'set' for i in percentile_columns]\n",
    "    result_percentiles_set.columns = ['group'] + percentile_columns_tmp\n",
    "\n",
    "    # get value percentiles\n",
    "    result_percentiles = df[(df['optype'] == 0) & (df['value'] > 0)].groupby('group')['value'].apply(custom_percentiles4)\n",
    "    if result_percentiles.empty:\n",
    "        result_percentiles = pd.DataFrame(0, index=all_groups, columns=percentile_columns).reset_index()\n",
    "        result_percentiles.rename(columns={'index': 'group'}, inplace=True)\n",
    "    else:\n",
    "        result_percentiles = result_percentiles.unstack(level=-1).reset_index()\n",
    "    percentile_columns_tmp = [i+'get' for i in percentile_columns]\n",
    "    result_percentiles.columns = ['group'] + percentile_columns_tmp   \n",
    "\n",
    "    if type == 'full':\n",
    "        df['bins']=df[(df['optype'] == 0) & (df['value'] > 0)].groupby('group')['Unique_Value_Sum'].transform(quantile_bins, num_bins=4)\n",
    "        result_percentiles_Unique_Value_Sum = df[(df['optype'] == 0) & (df['value'] > 0)].groupby(['group','bins'])['value'].apply(custom_percentiles4)\n",
    "        tmp=pd.DataFrame(result_percentiles_Unique_Value_Sum.unstack(level=-1)).reset_index().drop('bins',axis=1)\n",
    "        def funx(x):\n",
    "            return pd.Series(x.iloc[:,1:].values.reshape(1,-1)[0])\n",
    "        kkk=tmp.groupby('group').apply(lambda x:funx(x)).reset_index() \n",
    "        kkk=kkk.drop(0,axis=1)\n",
    "        kkk.columns=['group']+[str(i) for i in kkk.columns[1:]]\n",
    "\n",
    "    final_result = result_zero_ratio\n",
    "    final_result = final_result.merge(get_num, on='group', how='left')\n",
    "    final_result = final_result.merge(getnull_num, on='group', how='left')\n",
    "    final_result = final_result.merge(set_num, on='group', how='left')\n",
    "\n",
    "    final_result = final_result.merge(result_percentiles, on='group', how='left')\n",
    "    if type == 'full':\n",
    "        final_result = final_result.merge(kkk, on='group', how='left')\n",
    "    final_result = final_result.merge(result_percentiles_set, on='group', how='left')\n",
    "    final_result = final_result.merge(result_percentiles_sum, on='group', how='left')\n",
    "    final_result = final_result.merge(result_nullpercentiles_sum, on='group', how='left')\n",
    "    final_result = final_result.merge(get_key_range, on='group', how='left')\n",
    "\n",
    "    final_result=final_result.fillna(0)\n",
    "    return final_result\n",
    "    # final_result.iloc[4999:5000,:]\n",
    "    \n",
    "    \n",
    "def crps(y_true, y_pred_mean, y_pred_std):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    y_true: 实际值\n",
    "    y_pred_mean: 预测均值\n",
    "    y_pred_std: 预测标准差\n",
    "    \n",
    "    return:\n",
    "    crps_score\n",
    "    \"\"\"\n",
    "    # 标准化实际值\n",
    "    z = (y_true - y_pred_mean) / y_pred_std\n",
    "    \n",
    "    # 计算 CRPS\n",
    "    crps_score = y_pred_std * (z * (2 * norm.cdf(z) - 1) + 2 * norm.pdf(z) - 1 / np.sqrt(np.pi))\n",
    "    return crps_score\n",
    "\n",
    "\n",
    "def fold_prediction(model,model_name):\n",
    "\n",
    "    temp_train = X_train_or\n",
    "    # x_train, y_train = temp_train[use_cols], temp_train[['label','std']]\n",
    "    x_train, y_train = temp_train[use_cols], temp_train['label']     \n",
    "    model.fit(x_train, y_train)\n",
    "    with open(basepath+'model/'+model_name, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "def kfold_prediction(model,model_name):\n",
    "    pred_test = np.zeros(len(X_test_or))\n",
    "    pred_train = np.zeros(len(X_train_or))\n",
    "    \n",
    "    for index, (tr_idx, va_idx) in enumerate(kfold.split(all_index)):\n",
    "        temp_train = X_train_or.iloc[tr_idx]\n",
    "        temp_valid = X_train_or.iloc[va_idx]\n",
    "        # x_train, y_train = temp_train[use_cols], temp_train[['label','std']]\n",
    "        # x_valid, y_valid = temp_valid[use_cols], temp_valid[['label','std']]\n",
    "        x_train, y_train = temp_train[use_cols], temp_train['label']\n",
    "        x_valid, y_valid = temp_valid[use_cols], temp_valid['label']\n",
    "        model.fit(x_train, y_train)\n",
    "        pred_val = model.predict(x_valid)\n",
    "        temp_valid['pre']=pred_val\n",
    "        temp_valid['mape']=calculate_ape(y_valid,pred_val)\n",
    "\n",
    "        pred_test+=model.predict(test_x)/foldnum\n",
    "        pred_train[va_idx]=pred_val\n",
    "\n",
    "        ape_values = calculate_ape(y_valid, pred_val)\n",
    "        mape_test=np.sort(ape_values)[:].mean()\n",
    "        ape_p95 = np.percentile(ape_values,95)\n",
    "        ape_p99 = np.percentile(ape_values,99)\n",
    "\n",
    "        print(f\"{model_name} val:{index}, MAPE：{mape_test} p95_ape:{ape_p95} p99_ape:{ape_p99}\")\n",
    "    \n",
    "    ape_values = calculate_ape(X_test_or['label'], pred_test)\n",
    "    mape_test=np.sort(ape_values)[:].mean()\n",
    "    percentile_95 = np.percentile(ape_values, 95)\n",
    "    percentile_99 = np.percentile(ape_values, 99)\n",
    "    print(f\"{model_name} test: ,mean MAPE:{mape_test}, P95ape:{percentile_95}, P99ape:{percentile_99}\")\n",
    "    \n",
    "    # 获取特征重要性\n",
    "    feature_names = X_train_or[use_cols].columns\n",
    "    if (model_name=='CatBoostRegressor'):\n",
    "        feature_importances = model.get_feature_importance()\n",
    "    elif(model_name in ['XGBRegressor','LGBMRegressor']):\n",
    "        feature_importances = model.feature_importances_\n",
    "    # 创建一个DataFrame来展示特征名和它们的重要性\n",
    "    feature_importance_df = pd.DataFrame({'Feature Name': feature_names, 'Importance': feature_importances})\n",
    "    # 按重要性降序排列\n",
    "    feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "    print(feature_importance_df)\n",
    "    \n",
    "all_filter_index = []\n",
    "def noise_filter(row,noisetype ='b',alg = 'iqr',allindex = all_filter_index):\n",
    "    if alg == 'iqr':\n",
    "        #四分位点\n",
    "        Q1 = np.percentile(row, 25)\n",
    "        Q3 = np.percentile(row, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # filtered_values = row[(row < upper_bound) &(row > lower_bound)]\n",
    "        filtered_values = row[row < upper_bound]\n",
    "        noise_b_list = row[row >= upper_bound]\n",
    "    elif alg == 'sigma':\n",
    "        #3sigma\n",
    "        mean = np.mean(row)\n",
    "        std = np.std(row)\n",
    "        filtered_values = row[row < mean+3*std]\n",
    "    elif alg == 'zscore':\n",
    "        #z-score\n",
    "        z_scores = zscore(row)\n",
    "        filtered_values = row[np.abs(z_scores)<3]\n",
    "    elif alg == 'lof':\n",
    "        row_reshaped = row.values.reshape(-1, 1)\n",
    "        lof = LocalOutlierFactor(n_neighbors=3)\n",
    "        lof_fit = lof.fit_predict(row_reshaped)\n",
    "        filtered_values = row[lof_fit == 1]\n",
    "        noise_b_list = row[lof_fit == -1]\n",
    "    elif alg == 'isof':\n",
    "        # Isolation Forest\n",
    "        row_reshaped = row.values.reshape(-1, 1)\n",
    "        iso_forest = IsolationForest(contamination=0.2)\n",
    "        iso_fit = iso_forest.fit_predict(row_reshaped)\n",
    "        filtered_values = row[iso_fit != -1]\n",
    "        noise_b_list = row[iso_fit == -1]\n",
    "    elif alg == 'mad':\n",
    "        # MAD (Median Absolute Deviation)\n",
    "        median = np.median(row)\n",
    "        mad = np.median([np.abs(x - median) for x in row])\n",
    "        modified_z_scores = [0.6745 * (x - median) / mad for x in row]\n",
    "        filtered_values = row[np.abs(modified_z_scores) < 3.5]\n",
    "        noise_b_list = row[np.abs(modified_z_scores) >= 3.5]\n",
    "    elif alg == 'dbscan':\n",
    "        # DBSCAN\n",
    "        row_reshaped = row.values.reshape(-1, 1)\n",
    "        db = DBSCAN(eps=0.3, min_samples=3)\n",
    "        db_fit = db.fit_predict(row_reshaped)\n",
    "        filtered_values = row[db_fit != -1]\n",
    "        noise_b_list = row[db_fit == -1]\n",
    "    \n",
    "    mean_val = np.mean(filtered_values)\n",
    "    std_val = np.std(filtered_values)\n",
    "    if noisetype=='a':\n",
    "        filtered_values = filtered_values /mean_val\n",
    "        all_filter_index.extend(filtered_values.tolist())\n",
    "    elif noisetype=='b':\n",
    "        # filtered_values = (filtered_values - filtered_values)/mean_val\n",
    "        # all_filter_index.extend(filtered_values.tolist())\n",
    "        # all_filter_index.extend(noise_b_list.tolist())\n",
    "        noise_b_index = noise_b_list.index\n",
    "        return noise_b_index\n",
    "        \n",
    "    return mean_val\n",
    "\n",
    "class MAPEMetric(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return np.mean(error)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        # approxes 是预测值的列表，target 是真实值的列表\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "\n",
    "        approx = approxes[0]\n",
    "        error_sum = 0.0\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for i in range(len(approx)):\n",
    "            w = 1.0 if weight is None else weight[i]\n",
    "            error_sum += w * (np.abs(target[i] - approx[i]) / target[i])\n",
    "            total_weight += w\n",
    "\n",
    "        return error_sum / total_weight, total_weight\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = getfeature(recordfile,distancefile,basepath)\n",
    "labeltmp=pd.read_csv(basepath+'record'+clusterid+'.txtlable', sep=' ', header=None,names=['time'])\n",
    "labeltmp = labeltmp.groupby(labeltmp.index // (windowsize/1000)).sum()\n",
    "final_result['label'] = labeltmp['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_or, X_test_or, y_train_or, y_test_or=train_test_split(final_result, final_result, test_size=0.2, random_state=5)\n",
    "X_train_or=X_train_or.reset_index(drop=True)\n",
    "X_test_or=X_test_or.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Kfold\n",
    "error_data=[]\n",
    "nerror_data=[]\n",
    "\n",
    "\n",
    "foldnum=5\n",
    "real_result_list=[]\n",
    "all_index = range(len(X_train_or))\n",
    "kfold = KFold(n_splits=foldnum, shuffle=True, random_state=4)\n",
    "use_cols = [col for col in final_result.columns if col not in ['group','d','label','std','bin','time']]\n",
    "label = 'time'  #一般是label\n",
    "# '1','2','3','4','5','6','7','8','9','10','11','12','13','14','15'\n",
    "def fold_prediction(model,model_name):\n",
    "\n",
    "    temp_train = X_train_or\n",
    "    # x_train, y_train = temp_train[use_cols], temp_train[['label','std']]\n",
    "    x_train, y_train = temp_train[use_cols], temp_train[label]     \n",
    "    model.fit(x_train, y_train)\n",
    "    with open(basepath+'aliyunmodel/'+model_name, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "test_x = X_test_or[use_cols]\n",
    "def kfold_prediction(model,model_name):\n",
    "    pred_test = np.zeros(len(X_test_or))\n",
    "    pred_train = np.zeros(len(X_train_or))\n",
    "    \n",
    "    for index, (tr_idx, va_idx) in enumerate(kfold.split(all_index)):\n",
    "        temp_train = X_train_or.iloc[tr_idx]\n",
    "        temp_valid = X_train_or.iloc[va_idx]\n",
    "        # x_train, y_train = temp_train[use_cols], temp_train[['label','std']]\n",
    "        # x_valid, y_valid = temp_valid[use_cols], temp_valid[['label','std']]\n",
    "        x_train, y_train = temp_train[use_cols], temp_train[label]\n",
    "        x_valid, y_valid = temp_valid[use_cols], temp_valid[label]\n",
    "        model.fit(x_train, y_train)\n",
    "        pred_val = model.predict(x_valid)\n",
    "        temp_valid['pre']=pred_val\n",
    "        temp_valid['mape']=calculate_ape(y_valid,pred_val)\n",
    "\n",
    "        pred_test+=model.predict(test_x)/foldnum\n",
    "        pred_train[va_idx]=pred_val\n",
    "\n",
    "        ape_values = calculate_ape(y_valid, pred_val)\n",
    "        mape_test=np.sort(ape_values)[:].mean()\n",
    "        ape_p95 = np.percentile(ape_values,95)\n",
    "        ape_p99 = np.percentile(ape_values,99)\n",
    "\n",
    "        print(f\"{model_name} val:{index}, MAPE：{mape_test} p95_ape:{ape_p95} p99_ape:{ape_p99}\")\n",
    "    \n",
    "    ape_values = calculate_ape(X_test_or[label], pred_test)\n",
    "    mape_test=np.sort(ape_values)[:].mean()\n",
    "    percentile_95 = np.percentile(ape_values, 95)\n",
    "    percentile_99 = np.percentile(ape_values, 99)\n",
    "    print(f\"{model_name} test: ,mean MAPE:{mape_test}, P95ape:{percentile_95}, P99ape:{percentile_99}\")\n",
    "    \n",
    "    # 获取特征重要性\n",
    "    feature_names = X_train_or[use_cols].columns\n",
    "    if (model_name=='CatBoostRegressor'):\n",
    "        feature_importances = model.get_feature_importance()\n",
    "    elif(model_name in ['XGBRegressor','LGBMRegressor']):\n",
    "        feature_importances = model.feature_importances_\n",
    "    # 创建一个DataFrame来展示特征名和它们的重要性\n",
    "    feature_importance_df = pd.DataFrame({'Feature Name': feature_names, 'Importance': feature_importances})\n",
    "    # 按重要性降序排列\n",
    "    feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "    print(feature_importance_df)\n",
    "\n",
    "\n",
    "def calculate_ape(y_true, y_pred):\n",
    "    \"\"\" Calculate Absolute Percentage Error for each prediction \"\"\"\n",
    "    return np.abs((y_true - y_pred) / y_true)\n",
    "\n",
    "def calculate_mape(row):\n",
    "    actual = row[:-2]  # 最后两列是均值和方差，不包括在实际值中\n",
    "    mean_value = row['mean']\n",
    "    ape = calculate_ape(actual, mean_value)\n",
    "    mape = ape.mean()\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemode = 1\n",
    "model = LGBMRegressor(verbosity=-1,\n",
    "                      metric='mse',\n",
    "                      max_depth=8,\n",
    "                      n_estimators=1000,\n",
    "                      colsample_bytree=0.8,\n",
    "                      lambda_l1=0.1,\n",
    "                      objective='huber',\n",
    "                      alpha=0.95\n",
    "                      ) #metric：mse \n",
    "if (savemode):\n",
    "    fold_prediction(model,\"LGBM_base\")\n",
    "else:\n",
    "    kfold_prediction(model,\"LGBMRegressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    epsilon = 1e-7 \n",
    "    return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 确保cuDNN使用确定性算法\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "df = final_result.copy()\n",
    "use_cols = [col for col in final_result.columns if col not in ['group','label','std','bin','flag', 'time','normal','1', '2', '3', '4', '5', '6', '7', '8', '9', '10','11', '12', '13', '14', '15',]]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 1. 准备数据\n",
    "#\n",
    "\n",
    "X = df[use_cols].values  # feature\n",
    "y = df['label'].values  # label\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)  # 转换为列向量\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "empty_x_cat_train = torch.empty((X_train.shape[0], 0), dtype=torch.long).to(device)\n",
    "empty_x_cat_test = torch.empty((X_test.shape[0], 0), dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "class ContinuousTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=6, nhead=8, dim_feedforward=64, dropout=0.1):\n",
    "        super(ContinuousTransformer, self).__init__()\n",
    "        # 将每个特征单独投影\n",
    "        self.input_projection = nn.Linear(1, dim_feedforward)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_feedforward,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True  # 确保batch在第一个维度\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(dim_feedforward, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x_cont):\n",
    "        # x_cont形状: [batch_size, input_dim]\n",
    "        # 为每个特征添加一个维度\n",
    "        x_cont = x_cont.unsqueeze(2)  # 形状变为 [batch_size, input_dim, 1]\n",
    "        x = self.input_projection(x_cont)  # 形状为 [batch_size, input_dim, dim_feedforward]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_encoder(x)  \n",
    "        x = x.mean(dim=1)  # 聚合后形状为 [batch_size, dim_feedforward]\n",
    "        output = self.output_layer(x)  # 输出形状为 [batch_size, 1]\n",
    "        return output\n",
    "\n",
    "\n",
    "# 获取输入特征的维度\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "# 实例化模型\n",
    "model = ContinuousTransformer(input_dim=input_dim, num_layers=4, nhead=16, dim_feedforward=32, dropout= 0.0004058734643191623).to(device)\n",
    "\n",
    "\n",
    "# 3. 定义损失函数和优化器\n",
    "# criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3.694663548346235e-05, weight_decay=1.6337705553617178e-05)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 80 # 训练轮数\n",
    "batch_size = 16  # 每个batch大小 32\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # tolerant round\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # outputs = model(inputs, x_cat=empty_x_cat_train)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_predictions = model(X_test)\n",
    "        # train_predictions = model(X_test, x_cat=empty_x_cat_test)\n",
    "        val_loss = criterion(train_predictions, y_test)\n",
    "        train_mape = mean_absolute_percentage_error(y_test, train_predictions)\n",
    "        \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"验证集性能不再提升，提前停止训练\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}-------{val_loss}, MAPE: {train_mape:.2f}%')\n",
    "\n",
    "# 4. evaluattion model\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    # predictions = model(X_test,x_cat=empty_x_cat_test)\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test MAPE: {test_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gpytorch.kernels import RBFKernel, PeriodicKernel, SpectralMixtureKernel, ScaleKernel\n",
    "\n",
    "# Setting the random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)  \n",
    "torch.manual_seed(seed)  \n",
    "random.seed(seed)  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    \n",
    "\n",
    "# feature_columns = ['optype', 'get_ct', 'getnull_ct', 'set_ct', '25%set', '50%set', '75%set', '100%set', 'key', 'time']\n",
    "feature_columns = ['optype', 'get_ct', 'getnull_ct', 'set_ct', '25%get', '50%get',\n",
    "       '75%get', '100%get', '25%set', '50%set', '75%set', '100%set',\n",
    "       '25%sum', '50%sum', '75%sum', '90%sum', '95%sum', '99%sum','key', 'time']\n",
    "# feature_columns = ['optype', 'get_ct', 'getnull_ct', 'set_ct', '25%get', '50%get',\n",
    "#        '75%get', '100%get', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "#        '11', '12', '13', '14', '15', '25%set', '50%set', '75%set', '100%set',\n",
    "#        '25%sum', '50%sum', '75%sum', '90%sum', '95%sum', '99%sum',\n",
    "#        '25%null_sum', '50%null_sum', '75%null_sum', '90%null_sum',\n",
    "#        '95%null_sum', '99%null_sum', 'key', 'time']\n",
    "# feature_columns = [col for col in final_result.columns if col not in ['group', 'label', 'flag','ratio']]\n",
    "\n",
    "# test CUDA \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "final_result[feature_columns] = scaler_X.fit_transform(final_result[feature_columns])\n",
    "\n",
    "\n",
    "percentile_99 = np.percentile(final_result['label'], 99.9)\n",
    "filtered_labels = final_result[final_result['label'] <= percentile_99]['label']\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(filtered_labels.values.reshape(-1, 1))\n",
    "final_result['label'] = scaler_y.transform(final_result['label'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "#  LSTM model\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(LSTMFeatureExtractor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.ln = nn.LayerNorm(output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        final_output = lstm_out[:, -1, :]\n",
    "        fc_output = self.fc(final_output)\n",
    "        normalized_output = self.ln(fc_output)\n",
    "        return normalized_output\n",
    "    \n",
    "input_dim = len(feature_columns)\n",
    "lstm_extractor = LSTMFeatureExtractor(input_dim, hidden_dim=64, output_dim=64).to(device)\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "zero_indices = []\n",
    "non_zero_indices = []\n",
    "#slide window size\n",
    "n = 10\n",
    "sequence_length = 16000  \n",
    "\n",
    "\n",
    "'''\n",
    "使用LSTM-----------\n",
    "'''\n",
    "for start in range(0, len(final_result), sequence_length):\n",
    "    sequence_data = final_result.iloc[start:start + sequence_length].copy()\n",
    "    for i in range(len(sequence_data) - n):\n",
    "        current_window = sequence_data.iloc[i:i + n][feature_columns].values\n",
    "        current_window_tensor = torch.tensor(current_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        lstm_feature = lstm_extractor(current_window_tensor)  \n",
    "        # if(sequence_data.iloc[i + n - 1]['label']>5):\n",
    "        #     continue\n",
    "        X.append(lstm_feature.detach().cpu().numpy().flatten())  \n",
    "        y.append(sequence_data.iloc[i + n - 1]['label'])\n",
    "        \n",
    "        if sequence_data.iloc[i + n-1]['flag'] == 0:\n",
    "            zero_indices.append(len(y) - 1)  \n",
    "        elif sequence_data.iloc[i + n-1]['flag'] == 1:\n",
    "            non_zero_indices.append(len(y) - 1)\n",
    "\n",
    "\n",
    "'''\n",
    "concat\n",
    "'''\n",
    "# for start in range(0, len(final_result), sequence_length):\n",
    "#     sequence_data = final_result.iloc[start:start + sequence_length].copy()\n",
    "#     for i in range(len(sequence_data) - n):\n",
    "#         # concat feature\n",
    "#         current_window = sequence_data.iloc[i:i + n][feature_columns].values.flatten()  \n",
    "#         X.append(current_window)\n",
    "#         y.append(sequence_data.iloc[i + n - 1]['label'])  \n",
    "        \n",
    "#         if sequence_data.iloc[i + n - 1]['flag'] == 0:\n",
    "#             zero_indices.append(len(y) - 1)  \n",
    "#         elif sequence_data.iloc[i + n - 1]['flag'] == 1:\n",
    "#             non_zero_indices.append(len(y) - 1)\n",
    "            \n",
    "'''\n",
    "mean-pooling\n",
    "'''\n",
    "# for start in range(0, len(final_result), sequence_length):\n",
    "#     sequence_data = final_result.iloc[start:start + sequence_length].copy()\n",
    "#     for i in range(len(sequence_data) - n):\n",
    "#         current_window = sequence_data.iloc[i:i + n][feature_columns].values  # (n, feature_dim)\n",
    "#         mean_pooled_features = np.mean(current_window, axis=0) \n",
    "#         X.append(mean_pooled_features)\n",
    "#         y.append(sequence_data.iloc[i + n - 1]['label'])  \n",
    "        \n",
    "#         if sequence_data.iloc[i + n - 1]['flag'] == 0:\n",
    "#             zero_indices.append(len(y) - 1)  \n",
    "#         elif sequence_data.iloc[i + n - 1]['flag'] == 1:\n",
    "#             non_zero_indices.append(len(y) - 1)\n",
    "\n",
    "'''\n",
    "no encoder \n",
    "'''\n",
    "# for start in range(0, len(final_result), sequence_length):\n",
    "#     sequence_data = final_result.iloc[start:start + sequence_length].copy()\n",
    "#     for i in range(len(sequence_data) - n):\n",
    "#         current_window = sequence_data.iloc[i + n-1][feature_columns].values  # (n, feature_dim)\n",
    "#         X.append(current_window)\n",
    "#         y.append(sequence_data.iloc[i + n - 1]['label']) \n",
    "        \n",
    "#         if sequence_data.iloc[i + n - 1]['flag'] == 0:\n",
    "#             zero_indices.append(len(y) - 1)  \n",
    "#         elif sequence_data.iloc[i + n - 1]['flag'] == 1:\n",
    "#             non_zero_indices.append(len(y) - 1)\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"zero:{len(zero_indices)} nonzero:{len(non_zero_indices)}\")\n",
    "\n",
    "#下采样\n",
    "if len(zero_indices) > len(non_zero_indices):\n",
    "    downsampled_zero_indices = np.random.choice(zero_indices, size=len(zero_indices), replace=False)# kvrocks/redisaof 1/10 1/30 cassandra 1/10\n",
    "    # downsampled_nonzero_indices = np.random.choice(non_zero_indices, size=len(non_zero_indices)//2, replace=False)\n",
    "    balanced_indices = np.concatenate([downsampled_zero_indices, non_zero_indices])\n",
    "    \n",
    "else:\n",
    "    balanced_indices = np.concatenate([zero_indices, non_zero_indices])\n",
    " \n",
    "X_balanced = X[balanced_indices]\n",
    "y_balanced = y[balanced_indices]\n",
    "\n",
    "\n",
    "X_balanced = torch.tensor(X_balanced, dtype=torch.float32).to(device)\n",
    "y_balanced = torch.tensor(y_balanced, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 定义 GPyTorch 模型\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        input_dim = train_x.shape[-1]\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            RBFKernel(ard_num_dims=input_dim)\n",
    "        )\n",
    "        #PeriodicKernel(ard_num_dims=input_dim) + SpectralMixtureKernel(num_mixtures=4, ard_num_dims=input_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# likelihood function\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)#同方差假设\n",
    "model = GPRegressionModel(X_train, y_train, likelihood).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# training model\n",
    "model.train()\n",
    "lstm_extractor.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': lstm_extractor.parameters()} \n",
    "], lr=0.005)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 100\n",
    "\n",
    "print(\"Training the GPyTorch model...\")\n",
    "with tqdm(total=training_iterations) as pbar:\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(X_train) #X_train\n",
    "        \n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        pbar.update(1)\n",
    "    \n",
    "\n",
    "# prediction\n",
    "model.eval()\n",
    "lstm_extractor.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "print(\"Predicting on the training set...\")\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    train_pred = likelihood(model(X_train))\n",
    "    y_train_pred = train_pred.mean\n",
    "    sigma_train = train_pred.stddev\n",
    "\n",
    "print(\"Predicting on the test set...\")\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_pred = likelihood(model(X_test))\n",
    "    y_test_pred = test_pred.mean\n",
    "    sigma_test = test_pred.stddev\n",
    "\n",
    "y_train_pred = y_train_pred.cpu().numpy()\n",
    "sigma_train = sigma_train.cpu().numpy()\n",
    "y_test_pred = y_test_pred.cpu().numpy()\n",
    "sigma_test = sigma_test.cpu().numpy()\n",
    "\n",
    "y_train = y_train.cpu().numpy()\n",
    "y_test = y_test.cpu().numpy()\n",
    "\n",
    "\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred.reshape(-1, 1)).flatten()\n",
    "sigma_train_rescaled = sigma_train * scaler_y.scale_\n",
    "sigma_test_rescaled = sigma_test * scaler_y.scale_\n",
    "\n",
    "y_train_rescaled = scaler_y.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "# Visualisation of predicted results\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(range(len(y_train)), y_train_rescaled, label='Real Train')\n",
    "plt.scatter(range(len(y_train)), y_train_pred_rescaled, label='Predicted Train', marker='x')\n",
    "plt.fill_between(range(len(y_train)), y_train_pred_rescaled - 1.645 * sigma_train_rescaled, y_train_pred_rescaled + 1.645 * sigma_train_rescaled, alpha=0.2)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Train Data: Real vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(range(len(y_test)), y_test_rescaled, label='Real Test')\n",
    "plt.scatter(range(len(y_test)), y_test_pred_rescaled, label='Predicted Test', marker='x')\n",
    "plt.fill_between(range(len(y_test)), y_test_pred_rescaled - 1.645 * sigma_test_rescaled, y_test_pred_rescaled + 1.645 * sigma_test_rescaled, alpha=0.2)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Test Data: Real vs Predicted')\n",
    "plt.legend()\n",
    "plt.ylim(0,20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
